{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0c761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8832cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e348e30661d34e3e9a07c0400197891a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\akash\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer and model (using \"bert-base-multilingual-cased\" model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)  # Adjust num_labels as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f37830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Data  Sentiment\n",
      "0                                   ভালোই কিনতু বইটা          1\n",
      "1  পড়লাম বইটা মাঝখানে কিছুটা বোরিং মনে হয়েছে অনুব...          0\n",
      "2  বোরিং বোরিং বোরং ছোট গলপটির বইটি নেওয়া উচিত ছ...          0\n",
      "3                                 বইটি চমৎকার লেগেছে          1\n",
      "4    অসাধারণ বই লজিকাল সকিল ডেভেলপমেনট জনযে অতুলনীয়          1\n"
     ]
    }
   ],
   "source": [
    "# Load your CSV file for training\n",
    "df = pd.read_csv(\"book_review_train.csv\")\n",
    "print(df.head())\n",
    "texts = df[\"Data\"].tolist()\n",
    "labels = df[\"Sentiment\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8577d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize your text data\n",
    "tokenized_texts = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenized_texts[\"input_ids\"][idx]\n",
    "        attention_mask = self.tokenized_texts[\"attention_mask\"][idx]\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": label\n",
    "        }\n",
    "\n",
    "train_dataset = CustomDataset(tokenized_texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20b62778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 148\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 57\n",
      "  Number of trainable parameters = 177854978\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 09:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./sentiment_model_bert_base\n",
      "Configuration saved in ./sentiment_model_bert_base\\config.json\n",
      "Model weights saved in ./sentiment_model_bert_base\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    output_dir=\"./sentiment_model_bert_base\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Create a DataLoader for training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Initialize Trainer for fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26897096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer (using \"bert-base-multilingual-cased\" model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"C:\\\\Users\\\\akash\\\\sentiment_model_bert_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86a39c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your test CSV file\n",
    "df_test = pd.read_csv(\"book_review_test.csv\")  # Replace with the path to your test dataset\n",
    "\n",
    "# Extract the test texts and labels\n",
    "test_texts = df_test[\"Data\"].tolist()\n",
    "test_labels = df_test[\"Sentiment\"].tolist()\n",
    "\n",
    "# Tokenize the test text data\n",
    "tokenized_test_texts = tokenizer(test_texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfa02201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for test data\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenized_texts[\"input_ids\"][idx]\n",
    "        attention_mask = self.tokenized_texts[\"attention_mask\"][idx]\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": label\n",
    "        }\n",
    "\n",
    "test_dataset = CustomTestDataset(tokenized_test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc3d1727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80        22\n",
      "           1       0.80      0.92      0.86        26\n",
      "\n",
      "    accuracy                           0.83        48\n",
      "   macro avg       0.84      0.83      0.83        48\n",
      "weighted avg       0.84      0.83      0.83        48\n",
      "\n",
      "Confusion Matrix:\n",
      " [[16  6]\n",
      " [ 2 24]]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for test data\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "model.eval()\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Convert logits to predicted labels\n",
    "        predicted = torch.argmax(logits, dim=1)\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "classification_rep = classification_report(true_labels, predicted_labels)\n",
    "confusion_mtx = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mtx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c772b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: বাংলাদেশ একটি সুন্দর দেশ\n",
      "Sentiment: positive\n",
      "Positive Percentage: 95.10%\n",
      "Negative Percentage: 4.90%\n",
      "\n",
      "\n",
      "Sentence:  এখানে সুন্দর প্রদেশের দৃশ্য, সহনীয় মানুষ, এবং প্রাচীন ঐতিহ্য রয়েছে\n",
      "Sentiment: positive\n",
      "Positive Percentage: 78.12%\n",
      "Negative Percentage: 21.88%\n",
      "\n",
      "\n",
      "Sentence:  বাংলাদেশে বর্ষা আসলে সবকিছু সবুজে লিপটে যায়\n",
      "Sentiment: positive\n",
      "Positive Percentage: 66.09%\n",
      "Negative Percentage: 33.91%\n",
      "\n",
      "\n",
      "Sentence:  এখানে পাট, জুতা, তাঁত, বাংলাদেশের প্রধান কৃষি ও শিল্পক্ষেত্রের প্রধান আয়োর উৎস\n",
      "Sentiment: positive\n",
      "Positive Percentage: 83.62%\n",
      "Negative Percentage: 16.38%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input paragraph\n",
    "input_paragraph = \"বাংলাদেশ একটি সুন্দর দেশ। এখানে সুন্দর প্রদেশের দৃশ্য, সহনীয় মানুষ, এবং প্রাচীন ঐতিহ্য রয়েছে। বাংলাদেশে বর্ষা আসলে সবকিছু সবুজে লিপটে যায়। এখানে পাট, জুতা, তাঁত, বাংলাদেশের প্রধান কৃষি ও শিল্পক্ষেত্রের প্রধান আয়োর উৎস।\"\n",
    "\n",
    "# Split the paragraph into sentences using \"।\" and \"?\"\n",
    "sentences = re.split(r'[।?]', input_paragraph)\n",
    "\n",
    "# Define a function to get sentiment, positive percentage, and negative percentage for a sentence\n",
    "def get_sentiment_and_percentages(input_text):\n",
    "    tokenized_input = tokenizer(input_text, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    output = model(**tokenized_input, return_dict=True)\n",
    "\n",
    "    # Get the probability distribution over classes\n",
    "    probabilities = torch.softmax(output.logits, dim=1)\n",
    "    positive_percentage = probabilities[0, 1].item() * 100  # Percentage for the positive class\n",
    "    negative_percentage = probabilities[0, 0].item() * 100  # Percentage for the negative class\n",
    "\n",
    "    # Map the predicted label to sentiment\n",
    "    predicted_label = output.logits.argmax().item()\n",
    "    sentiment_mapping = {0: \"negative\", 1: \"positive\"}\n",
    "    sentiment = sentiment_mapping[predicted_label]\n",
    "\n",
    "    return sentiment, positive_percentage, negative_percentage\n",
    "\n",
    "# Analyze the sentiment, positive percentage, and negative percentage of each sentence\n",
    "for sentence in sentences:\n",
    "    # Skip empty sentences\n",
    "    if not sentence.strip():\n",
    "        continue\n",
    "\n",
    "    sentiment, positive_percentage, negative_percentage = get_sentiment_and_percentages(sentence)\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(f\"Positive Percentage: {positive_percentage:.2f}%\")\n",
    "    print(f\"Negative Percentage: {negative_percentage:.2f}%\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b8cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe86c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\akash\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: requests in c:\\users\\akash\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\akash\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\akash\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\akash\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fee2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1a8eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=2)  # Adjust the number of labels as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21993b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Data  Sentiment\n",
      "0                                   ভালোই কিনতু বইটা          1\n",
      "1  পড়লাম বইটা মাঝখানে কিছুটা বোরিং মনে হয়েছে অনুব...          0\n",
      "2  বোরিং বোরিং বোরং ছোট গলপটির বইটি নেওয়া উচিত ছ...          0\n",
      "3                                 বইটি চমৎকার লেগেছে          1\n",
      "4    অসাধারণ বই লজিকাল সকিল ডেভেলপমেনট জনযে অতুলনীয়          1\n",
      "                                                  Data  Sentiment\n",
      "995                                     এককথায় অসাধারণ          1\n",
      "996  হুমায়ুন আহমেদ সযার উপনযাসটি এক অতুলনীয় সৃষটি...          1\n",
      "997  বইটিতে সতযিই হতাশ হয়েছি মনে হয়েছিল জীবনের কয...          0\n",
      "998  কেবল একগুচছ করেস অনাহত সমৃতি অরথহীন নিমন সতরের...          0\n",
      "999                                            ভালো বই          1\n"
     ]
    }
   ],
   "source": [
    "# Load your CSV file\n",
    "df = pd.read_csv(\"book_review_train.csv\")\n",
    "print(df.head())\n",
    "print(df.tail())\n",
    "texts = df[\"Data\"].tolist()\n",
    "labels = df[\"Sentiment\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e201b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize your text data\n",
    "tokenized_texts = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "034d3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenized_texts[\"input_ids\"][idx]\n",
    "        attention_mask = self.tokenized_texts[\"attention_mask\"][idx]\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": label\n",
    "        }\n",
    "\n",
    "train_dataset = CustomDataset(tokenized_texts, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a239106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 375\n",
      "  Number of trainable parameters = 278045186\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 1:17:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./sentiment_model\\checkpoint-100\n",
      "Configuration saved in ./sentiment_model\\checkpoint-100\\config.json\n",
      "Model weights saved in ./sentiment_model\\checkpoint-100\\pytorch_model.bin\n",
      "Saving model checkpoint to ./sentiment_model\\checkpoint-200\n",
      "Configuration saved in ./sentiment_model\\checkpoint-200\\config.json\n",
      "Model weights saved in ./sentiment_model\\checkpoint-200\\pytorch_model.bin\n",
      "Saving model checkpoint to ./sentiment_model\\checkpoint-300\n",
      "Configuration saved in ./sentiment_model\\checkpoint-300\\config.json\n",
      "Model weights saved in ./sentiment_model\\checkpoint-300\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./sentiment_model\n",
      "Configuration saved in ./sentiment_model\\config.json\n",
      "Model weights saved in ./sentiment_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    output_dir=\"./sentiment_model\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    save_total_limit=4,\n",
    ")\n",
    "\n",
    "# Create a DataLoader for training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Initialize Trainer for fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba838952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\akash/.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\77de1f7a7e5e737aead1cd880979d4f1b3af6668\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at C:\\Users\\akash/.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\77de1f7a7e5e737aead1cd880979d4f1b3af6668\\sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\akash/.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\77de1f7a7e5e737aead1cd880979d4f1b3af6668\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\akash/.cache\\huggingface\\hub\\models--xlm-roberta-base\\snapshots\\77de1f7a7e5e737aead1cd880979d4f1b3af6668\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading configuration file C:\\Users\\akash\\sentiment_model\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"C:\\\\Users\\\\akash\\\\sentiment_model\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file C:\\Users\\akash\\sentiment_model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at C:\\Users\\akash\\sentiment_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"C:\\\\Users\\\\akash\\\\sentiment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0973b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7976190476190477\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.74      0.80        91\n",
      "           1       0.74      0.87      0.80        77\n",
      "\n",
      "    accuracy                           0.80       168\n",
      "   macro avg       0.80      0.80      0.80       168\n",
      "weighted avg       0.81      0.80      0.80       168\n",
      "\n",
      "Confusion Matrix:\n",
      " [[67 24]\n",
      " [10 67]]\n"
     ]
    }
   ],
   "source": [
    "# Load your test CSV file\n",
    "df_test = pd.read_csv(\"book_review_test.csv\")  # Replace with the path to your test dataset\n",
    "\n",
    "# Extract the test texts and labels\n",
    "test_texts = df_test[\"Data\"].tolist()\n",
    "test_labels = df_test[\"Sentiment\"].tolist()\n",
    "\n",
    "# Tokenize the test text data\n",
    "tokenized_test_texts = tokenizer(test_texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Define a custom dataset class for test data\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenized_texts[\"input_ids\"][idx]\n",
    "        attention_mask = self.tokenized_texts[\"attention_mask\"][idx]\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": label\n",
    "        }\n",
    "\n",
    "test_dataset = CustomTestDataset(tokenized_test_texts, test_labels)\n",
    "\n",
    "# Create a DataLoader for test data\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "model.eval()\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Convert logits to predicted labels\n",
    "        predicted = torch.argmax(logits, dim=1)\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "classification_rep = classification_report(true_labels, predicted_labels)\n",
    "confusion_mtx = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mtx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d0c8108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: বাংলাদেশ একটি সুন্দর দেশ\n",
      "Sentiment: positive\n",
      "Positive Percentage: 90.79%\n",
      "Negative Percentage: 9.21%\n",
      "\n",
      "\n",
      "Sentence:  এখানে সুন্দর প্রদেশের দৃশ্য, সহনীয় মানুষ, এবং প্রাচীন ঐতিহ্য রয়েছে\n",
      "Sentiment: positive\n",
      "Positive Percentage: 80.48%\n",
      "Negative Percentage: 19.52%\n",
      "\n",
      "\n",
      "Sentence:  বাংলাদেশে বর্ষা আসলে সবকিছু সবুজে লিপটে যায়\n",
      "Sentiment: negative\n",
      "Positive Percentage: 48.85%\n",
      "Negative Percentage: 51.15%\n",
      "\n",
      "\n",
      "Sentence:  এখানে পাট, জুতা, তাঁত, বাংলাদেশের প্রধান কৃষি ও শিল্পক্ষেত্রের প্রধান আয়োর উৎস\n",
      "Sentiment: positive\n",
      "Positive Percentage: 77.44%\n",
      "Negative Percentage: 22.56%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input paragraph\n",
    "input_paragraph = \"বাংলাদেশ একটি সুন্দর দেশ। এখানে সুন্দর প্রদেশের দৃশ্য, সহনীয় মানুষ, এবং প্রাচীন ঐতিহ্য রয়েছে। বাংলাদেশে বর্ষা আসলে সবকিছু সবুজে লিপটে যায়। এখানে পাট, জুতা, তাঁত, বাংলাদেশের প্রধান কৃষি ও শিল্পক্ষেত্রের প্রধান আয়োর উৎস।\"\n",
    "\n",
    "# Split the paragraph into sentences using \"।\" and \"?\"\n",
    "sentences = re.split(r'[।?]', input_paragraph)\n",
    "\n",
    "# Define a function to get sentiment, positive percentage, and negative percentage for a sentence\n",
    "def get_sentiment_and_percentages(input_text):\n",
    "    tokenized_input = tokenizer(input_text, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    output = model(**tokenized_input, return_dict=True)\n",
    "\n",
    "    # Get the probability distribution over classes\n",
    "    probabilities = torch.softmax(output.logits, dim=1)\n",
    "    positive_percentage = probabilities[0, 1].item() * 100  # Percentage for positive class\n",
    "    negative_percentage = probabilities[0, 0].item() * 100  # Percentage for negative class\n",
    "\n",
    "    # Map the predicted label to sentiment\n",
    "    predicted_label = output.logits.argmax().item()\n",
    "    sentiment_mapping = {0: \"negative\", 1: \"positive\"}\n",
    "    sentiment = sentiment_mapping[predicted_label]\n",
    "\n",
    "    return sentiment, positive_percentage, negative_percentage\n",
    "\n",
    "# Analyze the sentiment, positive percentage, and negative percentage of each sentence\n",
    "for sentence in sentences:\n",
    "    # Skip empty sentences\n",
    "    if not sentence.strip():\n",
    "        continue\n",
    "\n",
    "    sentiment, positive_percentage, negative_percentage = get_sentiment_and_percentages(sentence)\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(f\"Positive Percentage: {positive_percentage:.2f}%\")\n",
    "    print(f\"Negative Percentage: {negative_percentage:.2f}%\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c109eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2a564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d1f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
